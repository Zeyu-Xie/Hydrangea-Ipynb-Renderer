{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Random Forest\n",
    "#### **Objective**\n",
    "In this lab, you will implement missing parts of a `RandomForest` classifier. This classifier is an ensemble of decision trees that makes predictions using bootstrap sampling and random feature selection.\n",
    "\n",
    "#### **Overview**\n",
    "A `RandomForest` model is built using the following steps:\n",
    "1. **Bootstrap Sampling**: Randomly selecting samples with replacement to train each tree.\n",
    "2. **Random Feature Selection**: Choosing a subset of features for training each tree.\n",
    "3. **Training Decision Trees**: Fitting multiple decision trees using the sampled dataset.\n",
    "4. **Aggregating Predictions**: Using majority voting to determine the final prediction.\n",
    "\n",
    "Your task is to implement the missing parts (marked as `TODO`) in the given code.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement Bootstrap Sampling**\n",
    "- **Function to complete**: `_bootstrap_sample(self, X, y)`\n",
    "- **Purpose**: Generate a bootstrap sample by randomly selecting data points **with replacement**.\n",
    "- **Implementation details**:\n",
    "  - Randomly sample `len(X)` instances from `X` and `y`, allowing repetition.\n",
    "  - Return:\n",
    "    - `X_sample`: The sampled feature matrix.\n",
    "    - `y_sample`: The corresponding labels.\n",
    "    - `indices`: The indices of the selected samples.\n",
    "- **Steps**:\n",
    "  1. Use `np.random.choice()` to sample indices from `X`.\n",
    "  2. Select the corresponding rows from `X` and `y` using the sampled indices.\n",
    "  3. Return `(X_sample, y_sample, indices)`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `replace=True` in `np.random.choice()` to allow repeated selections.\n",
    "- The number of selected indices should be equal to the number of rows in `X`.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement Random Feature Selection**\n",
    "- **Function to complete**: `_select_random_features(self, X, n_features)`\n",
    "- **Purpose**: Select a subset of features randomly to be used for training each tree.\n",
    "- **Implementation details**:\n",
    "  - Determine the number of features to select based on `self.max_features`:\n",
    "    - `\"sqrt\"` → `sqrt(n_features)`\n",
    "    - `\"log2\"` → `log2(n_features)`\n",
    "    - `int` value → Use the given number of features.\n",
    "  - Randomly select `max_features` feature indices.\n",
    "  - Return:\n",
    "    - `X_subset`: The feature matrix containing only the selected features.\n",
    "    - `feature_indices`: The indices of the selected features.\n",
    "- **Steps**:\n",
    "  1. Compute the number of features to select (`max_features`).\n",
    "  2. Randomly choose `max_features` feature indices using `np.random.choice()`.\n",
    "  3. Extract the corresponding columns from `X` using these indices.\n",
    "  4. Return `(X_subset, feature_indices)`.\n",
    "\n",
    "**Hints:**\n",
    "- Ensure `max_features` does not exceed `n_features`.\n",
    "- Use `replace=False` in `np.random.choice()` to prevent selecting the same feature multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 3: Train Individual Decision Trees**\n",
    "- **Function to complete**: `fit(self, X, y)`\n",
    "- **Purpose**: Train `n_estimators` decision trees using bootstrap samples and random feature selection.\n",
    "- **Implementation details**:\n",
    "  - **For each tree:**\n",
    "    1. **Create a bootstrap sample** from `X` and `y` using `_bootstrap_sample()`.\n",
    "    2. **Select a subset of features** from the bootstrap sample using `_select_random_features()`.\n",
    "    3. **Train a `DecisionTreeClassifier`** using the subset of features.\n",
    "    4. Store the trained tree and selected feature indices.\n",
    "\n",
    "**Steps:**\n",
    "1. Convert `X` and `y` into NumPy arrays if they are Pandas DataFrames/Series.\n",
    "2. Initialize an empty list `self.trees` to store trained trees.\n",
    "3. For each tree (`self.n_estimators`):\n",
    "   - Call `_bootstrap_sample()` to generate a training sample.\n",
    "   - Call `_select_random_features()` to select features for training.\n",
    "   - Train a `DecisionTreeClassifier` on the sampled data.\n",
    "   - Store the trained tree and the selected feature indices in `self.trees`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `np.random.seed(self.seed)` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A basic implementation of a Random Forest algorithm for classification.\n",
    "\n",
    "    This class builds an ensemble of decision trees using bootstrap sampling and random feature selection. \n",
    "    It combines predictions from individual trees using majority voting to produce a robust classification model.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Number of decision trees in the forest.\n",
    "    max_depth : int or None\n",
    "        Maximum depth of each tree. If None, the trees grow until pure leaves or until reaching `min_samples_split`.\n",
    "    min_samples_split : int\n",
    "        Minimum number of samples required to split an internal node in a tree.\n",
    "    max_features : str or int\n",
    "        Number of features to consider when looking for the best split.\n",
    "        - \"sqrt\": Square root of the total number of features.\n",
    "        - \"log2\": Base-2 logarithm of the total number of features.\n",
    "        - int: A specific number of features to use.\n",
    "    seed : int or None\n",
    "        Random seed for reproducibility. If None, results will vary between runs.\n",
    "    trees : list\n",
    "        A list containing tuples of trained decision trees and their selected feature indices/names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, max_features=\"sqrt\", seed=None):\n",
    "        \"\"\"\n",
    "        Initializes the Random Forest classifier.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_estimators : int, default=10\n",
    "            Number of decision trees to build in the forest.\n",
    "        max_depth : int or None, default=None\n",
    "            Maximum depth of each decision tree. If None, trees grow until they are pure or cannot be split further.\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum number of samples required to split an internal node.\n",
    "        max_features : str or int, default=\"sqrt\"\n",
    "            Number of features to consider when splitting nodes:\n",
    "            - \"sqrt\": Square root of the total number of features.\n",
    "            - \"log2\": Base-2 logarithm of the total number of features.\n",
    "            - int: Specific number of features.\n",
    "        seed : int or None, default=None\n",
    "            Random seed for reproducibility. If None, results may vary across runs.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.seed = seed\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates a bootstrap sample of the input dataset by sampling with replacement.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        y : numpy.ndarray\n",
    "            Target vector with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple (X_sample, y_sample, indices) where:\n",
    "            - X_sample is the feature matrix of the bootstrap sample.\n",
    "            - y_sample is the target vector of the bootstrap sample.\n",
    "            - indices are the indices of the selected samples.\n",
    "        \"\"\"\n",
    "        # TODO implement this function\n",
    "        # Step 1: Use np.random.choice() to sample indices from X.\n",
    "        #indices = ...\n",
    "        indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
    "\n",
    "        # Step 2: Select the corresponding rows from X and y using the sampled indices.\n",
    "        #X_sample = ...\n",
    "        #y_sample = ...\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y[indices]\n",
    "\n",
    "        return X_sample, y_sample, indices\n",
    "\n",
    "    def _select_random_features(self, X, n_features):\n",
    "        \"\"\"\n",
    "        Selects a random subset of features to use for a decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        n_features : int\n",
    "            Total number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple (X_subset, feature_indices) where:\n",
    "            - X_subset is the feature matrix containing only the selected features.\n",
    "            - feature_indices are the indices of the selected features.\n",
    "        \"\"\"\n",
    "        # TODO implement this function\n",
    "        # Step 1: Compute the number of features to select (max_features).\n",
    "        if self.max_features == \"sqrt\":\n",
    "            # max_features = ...\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == \"log2\":\n",
    "            # max_features = ...\n",
    "            max_features = int(np.log2(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "\n",
    "        # Step 2: Randomly choose max_features feature indices using np.random.choice().\n",
    "        #feature_indices = ...\n",
    "        feature_indices = np.random.choice(n_features, max_features, replace=False)\n",
    "\n",
    "        # Step 3: Extract the corresponding columns from X using these indices.\n",
    "        #X_subset = ...\n",
    "        X_subset = X[:, feature_indices]\n",
    "        return X_subset, feature_indices\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the Random Forest model by creating an ensemble of decision trees.\n",
    "\n",
    "        Each tree is trained on a bootstrap sample of the data and a random subset of features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "        y : numpy.ndarray\n",
    "            Target vector with shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.feature_names = None\n",
    "\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # TODO implement this function\n",
    "            # Create bootstrap sample\n",
    "            # Step 1: Call _bootstrap_sample() to generate a training sample.\n",
    "            # X_sample, y_sample, _ = ...\n",
    "            X_sample, y_sample, _ = self._bootstrap_sample(X, y)\n",
    "\n",
    "            \n",
    "            # Step 2: Call _select_random_features() to select features for training.\n",
    "            # X_subset, feature_indices = ...\n",
    "            X_subset, feature_indices = self._select_random_features(X_sample, n_features)\n",
    "\n",
    "            # Step 3: Train a DecisionTreeClassifier on the sampled data.\n",
    "            # tree = ...\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_subset, y_sample)\n",
    "\n",
    "            # Step 4: Store the trained tree and the selected feature indices in self.trees.\n",
    "            selected_features = feature_indices\n",
    "            self.trees.append((tree, selected_features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for the input data using the trained Random Forest.\n",
    "\n",
    "        Predictions are made by aggregating votes from all individual trees (majority voting).\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Predicted class labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))\n",
    "        for i, (tree, selected_features) in enumerate(self.trees):\n",
    "            feature_indices = selected_features\n",
    "\n",
    "            predictions[:, i] = tree.predict(X[:, feature_indices])\n",
    "\n",
    "        return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=1, arr=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the class works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into a training and a test set\n",
    "X_mix, X_val, y_mix, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mix, y_mix, test_size=0.25, random_state=0, stratify=y_mix)\n",
    "rf = RandomForest(n_estimators=5, max_depth=5, max_features=\"sqrt\", seed=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: kernel functions\n",
    "\n",
    "#### **Objective**\n",
    "In this lab, you will implement various kernel functions used in Support Vector Machines (SVM) and other machine learning algorithms. These kernels transform input data into higher-dimensional spaces, allowing linear models to capture complex patterns.\n",
    "\n",
    "#### **Overview**\n",
    "You will implement the following kernel functions:\n",
    "1. **Linear Kernel**: Computes the dot product between two vectors.\n",
    "2. **Polynomial Kernel**: Introduces non-linearity by raising the dot product to a power.\n",
    "3. **Radial Basis Function (RBF) Kernel**: Uses an exponential function to measure similarity based on distance.\n",
    "4. **Sigmoid Kernel**: Mimics the behavior of neural networks with a hyperbolic tangent function.\n",
    "\n",
    "Your task is to implement these functions as described below.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement Linear Kernel**\n",
    "- **Function to complete**: `linear_kernel(x1, x2)`\n",
    "- **Purpose**: Computes the dot product of two vectors.\n",
    "- **Formula**:\n",
    "  $ k(x_1, x_2) = x_1^T x_2 $\n",
    "- **Steps**:\n",
    "  1. Compute the dot product using `np.dot()`.\n",
    "  2. Return the computed value.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement Polynomial Kernel**\n",
    "- **Function to complete**: `polynomial_kernel(x1, x2, degree=3, coef0=1)`\n",
    "- **Purpose**: Computes a polynomial kernel, which introduces non-linearity.\n",
    "- **Formula**:\n",
    "  $ k(x_1, x_2) = (x_1^T x_2 + coef_0)^{degree} $\n",
    "- **Steps**:\n",
    "  1. Compute the dot product using `np.dot()`.\n",
    "  2. Add `coef0` to the result.\n",
    "  3. Raise the sum to the power of `degree`.\n",
    "  4. Return the computed value.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 3: Implement Radial Basis Function (RBF) Kernel**\n",
    "- **Function to complete**: `rbf_kernel(x1, x2, gamma=0.1)`\n",
    "- **Purpose**: Computes an RBF kernel, measuring similarity based on Euclidean distance.\n",
    "- **Formula**:\n",
    "  $ k(x_1, x_2) = \\exp(-\\gamma ||x_1 - x_2||^2) $\n",
    "- **Steps**:\n",
    "  1. Compute the squared Euclidean distance using `np.linalg.norm()`.\n",
    "  2. Multiply the distance by `-gamma`.\n",
    "  3. Compute the exponential using `np.exp()`.\n",
    "  4. Return the computed value.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 4: Implement Sigmoid Kernel**\n",
    "- **Function to complete**: `sigmoid_kernel(x1, x2, alpha=0.1, coef0=0)`\n",
    "- **Purpose**: Computes a sigmoid kernel, similar to neural network activation functions.\n",
    "- **Formula**:\n",
    "  $ k(x_1, x_2) = \\tanh(\\alpha (x_1^T x_2) + coef_0) $\n",
    "- **Steps**:\n",
    "  1. Compute the dot product using `np.dot()`.\n",
    "  2. Multiply by `alpha` and add `coef0`.\n",
    "  3. Apply the `np.tanh()` function.\n",
    "  4. Return the computed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    \"\"\"\n",
    "    Compute the linear kernel between two vectors.\n",
    "    k(x1, x2) = x1.T @ x2\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "    return x1.T @ x2\n",
    "\n",
    "def polynomial_kernel(x1, x2, degree=3, coef0=1):\n",
    "    \"\"\"\n",
    "    Compute the polynomial kernel between two vectors.\n",
    "    k(x1, x2) = (x1.T @ x2 + coef0)^degree\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "    return (x1.T @ x2 + coef0) ** degree\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian (RBF) kernel between two vectors.\n",
    "    k(x1, x2) = exp(-gamma * ||x1 - x2||^2)\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "def sigmoid_kernel(x1, x2, alpha=0.1, coef0=0):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid kernel between two vectors.\n",
    "    k(x1, x2) = tanh(alpha * (x1.T @ x2) + coef0)\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "    return np.tanh(alpha * (x1.T @ x2) + coef0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if all the function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel: 32\n",
      "Polynomial Kernel (degree=3): 35937\n",
      "RBF Kernel (gamma=0.1): 0.06720551273974976\n",
      "Sigmoid Kernel (alpha=0.1, coef0=0): 0.9966823978396512\n"
     ]
    }
   ],
   "source": [
    "# Example vectors\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = np.array([4, 5, 6])\n",
    "\n",
    "# Compute kernels\n",
    "print(\"Linear Kernel:\", linear_kernel(x1, x2))\n",
    "print(\"Polynomial Kernel (degree=3):\", polynomial_kernel(x1, x2, degree=3, coef0=1))\n",
    "print(\"RBF Kernel (gamma=0.1):\", rbf_kernel(x1, x2, gamma=0.1))\n",
    "print(\"Sigmoid Kernel (alpha=0.1, coef0=0):\", sigmoid_kernel(x1, x2, alpha=0.1, coef0=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implementing SVM Kernel Selection and Hyperparameter Tuning\n",
    "#### **Objective**\n",
    "In this lab, you will implement two functions to help optimize Support Vector Machine (SVM) models. SVM is a powerful supervised learning algorithm used for classification and regression tasks. Your task is to implement missing parts of the functions to **select the best kernel** and **find the best hyperparameters** for an SVM classifier.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Overview**\n",
    "SVM models depend on various factors such as kernel functions and hyperparameters (`C`, `gamma`, etc.). Selecting the right configuration improves classification accuracy. In this lab, you will implement:\n",
    "\n",
    "1. **Kernel Selection (`find_best_kernel`)**:\n",
    "   - Train SVM models using different kernel functions.\n",
    "   - Evaluate each model’s performance on a test set.\n",
    "   - Return the best-performing kernel.\n",
    "\n",
    "2. **Hyperparameter Tuning (`find_best_svm_params`)**:\n",
    "   - Test multiple hyperparameter combinations.\n",
    "   - Train SVM models for each configuration.\n",
    "   - Return the best-performing parameter set.\n",
    "\n",
    "Your task is to complete the missing parts (marked as `TODO`) in the given functions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement Kernel Selection**\n",
    "- **Function to complete**: `find_best_kernel(kernel_list, X, y, test_size=0.2, random_state=0)`\n",
    "- **Purpose**: Determine the best SVM kernel by training models with different kernel functions and evaluating their accuracy.\n",
    "- **Implementation details**:\n",
    "  - **Split the dataset** into training and test sets using `train_test_split()`.\n",
    "  - **Train an SVM model** with each kernel from `kernel_list`.\n",
    "  - **Predict on the test set** and compute accuracy.\n",
    "  - **Return the kernel** with the highest accuracy.\n",
    "  \n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement Hyperparameter Tuning**\n",
    "- **Function to complete**: `find_best_svm_params(param_dict, X, y, test_size=0.2, random_state=0)`\n",
    "- **Purpose**: Find the best hyperparameter combination for an SVM model by testing different values and selecting the best-performing set.\n",
    "- **Implementation details**:\n",
    "  - **Generate all possible hyperparameter combinations** from `param_dict`.\n",
    "  - **Split the dataset** into training and test sets.\n",
    "  - **Train an SVM model** for each parameter combination.\n",
    "  - **Predict on the test set** and compute accuracy.\n",
    "  - **Return the best-performing hyperparameter set**.\n",
    "  \n",
    "**Hints:**\n",
    "- Use `SVC(**param_set)` to train the model with a given parameter combination.\n",
    "- Use `itertools.product(*param_dict.values())` to generate all parameter combinations.\n",
    "- Keep track of the best-performing parameter set and its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def find_best_kernel(kernel_list, X, y, test_size=0.2, random_state=0):\n",
    "    \"\"\"\n",
    "    Finds the best kernel for an SVM model using a single validation split.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel_list (list): List of kernel names to test (e.g., ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    - X (array-like): Feature matrix\n",
    "    - y (array-like): Target labels\n",
    "    - test_size (float): Proportion of the dataset to use as the test set (default = 0.2)\n",
    "    - random_state (int): Random seed for reproducibility (default = 0)\n",
    "\n",
    "    Returns:\n",
    "    - best_kernel (str): The kernel with the highest validation accuracy\n",
    "    - best_score (float): The highest validation accuracy\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "\n",
    "    # 1. Use `train_test_split()` to split `X` and `y` into training (1-test_size) and testing (test_size) sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    best_score = 0\n",
    "    best_kernel = None\n",
    "\n",
    "    # 2. Iterate over each kernel in `kernel_list`.\n",
    "    for kernel in kernel_list:\n",
    "        # Train the SVM model\n",
    "        # 3. Train an `SVC` model using the current kernel.\n",
    "        model = SVC(kernel=kernel)\n",
    "        # ...\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 4. Make predictions on the test set.\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 5. Calculate accuracy using `accuracy_score()`.\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # 6. Keep track of the kernel with the highest accuracy.\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_kernel = kernel\n",
    "\n",
    "    return best_kernel, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if find_best_kernel works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Kernel: rbf with Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "# Load dataset\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=0)\n",
    "\n",
    "X, y = X_circles,y_circles\n",
    "\n",
    "# Define list of kernels to test\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Find the best kernel\n",
    "best_kernel, best_score = find_best_kernel(kernels, X, y)\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel} with Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "\n",
    "def find_best_svm_params(param_dict, X, y, test_size=0.2, random_state=0):\n",
    "    \"\"\"\n",
    "    Finds the best SVM parameters using a single validation split.\n",
    "\n",
    "    Parameters:\n",
    "    - param_dict (dict): Dictionary containing parameter lists to test.\n",
    "                         Example: {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10]}\n",
    "    - X (array-like): Feature matrix\n",
    "    - y (array-like): Target labels\n",
    "    - test_size (float): Proportion of the dataset to use as the test set (default = 0.2)\n",
    "    - random_state (int): Random seed for reproducibility (default = 0)\n",
    "\n",
    "    Returns:\n",
    "    - best_params (dict): Dictionary of the best parameter combination\n",
    "    - best_score (float): The highest validation accuracy\n",
    "    \"\"\"\n",
    "    # TODO: implement the function\n",
    "\n",
    "    # 1. Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "\n",
    "    # 2. Extract the hyperparameter keys from `param_dict`.\n",
    "    keys = param_dict.keys()\n",
    "\n",
    "    # 3. Use `itertools.product()` to generate all possible parameter combinations.\n",
    "    for values in product(*param_dict.values()):\n",
    "        # 4. Iterate over each parameter combination:\n",
    "        #     - Train an `SVC` model with the given parameters.\n",
    "        #     - Make predictions on the test set.\n",
    "        #     - Compute accuracy.\n",
    "        #     - Store the best-performing parameter combination.\n",
    "        params = dict(zip(keys, values))\n",
    "        model = SVC(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "    # 5. Return the best hyperparameter set and its corresponding accuracy.\n",
    "\n",
    "    # Generate all possible combinations of parameters\n",
    "    param_values = list(param_dict.values())\n",
    "    param_combinations = product(*param_values)\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'kernel': 'rbf', 'C': 0.1} with Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=0)\n",
    "X, y = X_circles, y_circles\n",
    "\n",
    "# Define a dictionary of parameters to test\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "# Find the best SVM parameters\n",
    "best_params, best_score = find_best_svm_params(param_grid, X, y)\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params} with Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implementing Naïve Bayes Classifier\n",
    "\n",
    "\n",
    "#### **Objective**\n",
    "In this lab, you will implement a **Naïve Bayes Classifier** that assumes conditional independence among features. This classifier will be based on count-based probabilities and log probabilities to prevent underflow.\n",
    "\n",
    "#### **Overview**\n",
    "You will implement the following functions:\n",
    "1. **Fit Function (`fit`)**: Train the Naïve Bayes classifier by computing class priors and feature likelihoods.\n",
    "2. **Class Probability Function (`_class_probability`)**: Compute the log probability of a given class given an input sample.\n",
    "3. **Predict Function (`predict`)**: Predict class labels for new data samples using the computed probabilities.\n",
    "\n",
    "Your task is to complete these functions as described below.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Understanding Naïve Bayes**\n",
    "\n",
    "##### **Bayes' Theorem**\n",
    "Bayes' Theorem states:\n",
    "\n",
    "$$P(C|X) = \\frac{P(X|C) P(C)}{P(X)}$$\n",
    "\n",
    "where:\n",
    "- **Posterior probability (`P(C|X)`)**: The probability of class `C` given the feature set `X`. This is what we want to compute in classification.\n",
    "- **Likelihood (`P(X|C)`)**: The probability of observing the feature set `X` given that the class is `C`.\n",
    "- **Prior probability (`P(C)`)**: The probability of class `C` before observing any features.\n",
    "- **Evidence (`P(X)`)**: The total probability of observing the feature set `X`, across all possible classes.\n",
    "\n",
    "##### **Applying Bayes' Theorem in Naïve Bayes Classifier**\n",
    "In the Naïve Bayes classifier, we compute the probability of each class `C` given the feature set `X` and select the class with the highest probability. Using Bayes' Theorem:\n",
    "\n",
    "\n",
    "$$P(C|X) = \\frac{P(X|C) P(C)}{P(X)}$$\n",
    "\n",
    "##### **Ignoring `P(X)`**\n",
    "Since `P(X)` is the same for all possible classes, it does not affect which class has the highest probability. Thus, for classification purposes, we can ignore `P(X)` and compute only:\n",
    "\n",
    "\n",
    "$$P(C|X) \\propto P(X|C) P(C)$$\n",
    "\n",
    "This allows us to calculate the **relative probability** of each class without the need for `P(X)`, simplifying the computation.\n",
    "\n",
    "##### **Using Log Probabilities**\n",
    "When computing `P(X|C) * P(C)`, we multiply several probabilities together. However, probabilities are often very small, and multiplying many small numbers can lead to **numerical underflow** (values becoming too small for computers to handle accurately). To avoid this issue, we apply the **logarithm** to convert multiplication into addition:\n",
    "\n",
    "\n",
    "$$\\log (P(X|C) P(C)) = \\log P(C) + \\sum \\log P(X_i | C)$$\n",
    "\n",
    "##### **Laplace Smoothing**\n",
    "To handle cases where a feature value never appears in a given class (i.e., `P(X|C) = 0`), we apply **Laplace smoothing** (also known as **additive smoothing**). Laplace smoothing prevents zero probabilities by adding a small value (typically `1`) to all counts:\n",
    "\n",
    "\n",
    "$$P(X_i | C) = \\frac{count(X_i, C) + 1}{total(C) + |V|}$$\n",
    "\n",
    "\n",
    "where:\n",
    "- `count(X_i, C)`: The number of times feature `X_i` appears in class `C`.\n",
    "- `total(C)`: The total number of feature occurrences in class `C`.\n",
    "- `|V|`: The total number of unique feature values (vocabulary size in text classification).\n",
    "\n",
    "This ensures that no probability is ever zero, making the classifier more robust.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement `fit` Method**\n",
    "- **Function to complete**: `fit(self, X, y)`\n",
    "- **Purpose**: Train the Naïve Bayes classifier by computing class priors and feature likelihoods.\n",
    "- **Implementation Details**:\n",
    "  1. Identify unique class labels from `y`.\n",
    "  2. Count occurrences of each class label to compute prior probabilities.\n",
    "  3. Count occurrences of feature values for each class to compute likelihood probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement `_class_probability` Method**\n",
    "- **Function to complete**: `_class_probability(self, x, c)`\n",
    "- **Purpose**: Compute the log probability of class `c` given input sample `x`.\n",
    "- **Implementation Details**:\n",
    "  1. Compute the log prior probability of class `c`.\n",
    "  2. Compute the log likelihood of the input sample `x` given class `c`, using Laplace smoothing.\n",
    "  3. Sum log prior and log likelihood to get the final log probability.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 3: Implement `predict` Method**\n",
    "- **Function to complete**: `predict(self, X)`\n",
    "- **Purpose**: Predict class labels for input data `X`.\n",
    "- **Implementation Details**:\n",
    "  1. Compute the log probability for each class using `_class_probability`.\n",
    "  2. Choose the class with the highest log probability as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    A Naïve Bayes classifier that assumes conditional independence among features.\n",
    "    This implementation uses categorical (count-based) probabilities and log probabilities to prevent underflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize necessary data structures for the classifier.\n",
    "        - self.classes: Stores the unique class labels.\n",
    "        - self.class_counts: A dictionary to count occurrences of each class.\n",
    "        - self.feature_counts: A dictionary to count occurrences of feature values for each class.\n",
    "        - self.feature_totals: A dictionary to store total feature occurrences per class.\n",
    "        \"\"\"\n",
    "        self.classes = None  # Stores unique class labels\n",
    "        self.class_counts = {}  # Dictionary to store class counts\n",
    "        self.feature_counts = {}  # Dictionary to store feature counts per class\n",
    "        self.feature_totals = {}  # Dictionary to store total feature occurrences per class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Naïve Bayes classifier using count-based probabilities.\n",
    "        Steps:\n",
    "        1. Identify unique class labels from `y`.\n",
    "        2. Count occurrences of each class label to compute prior probabilities.\n",
    "        3. Count occurrences of feature values for each class to compute likelihood probabilities.\n",
    "        \n",
    "        :param X: 2D list or numpy array, where each row represents a sample and each column represents a feature.\n",
    "        :param y: 1D list or numpy array, where each element corresponds to the class label of a sample.\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)  # Step 1: Identify unique class labels\n",
    "        \n",
    "        for xi, label in zip(X, y):  # Iterate over each sample\n",
    "            # Step 2: Count class occurrences\n",
    "            # ...\n",
    "            if label not in self.class_counts:\n",
    "                self.class_counts[label] = 0\n",
    "            self.class_counts[label] += 1\n",
    "\n",
    "            \n",
    "            # Initialize feature count storage for each class if not present\n",
    "            # ...\n",
    "            if label not in self.feature_counts:\n",
    "                self.feature_counts[label] = {}\n",
    "            if label not in self.feature_totals:\n",
    "                self.feature_totals[label] = 0\n",
    "            \n",
    "            # Step 3: Count occurrences of each feature value for each class\n",
    "            # ...\n",
    "            for i, value in enumerate(xi):\n",
    "                if i not in self.feature_counts[label]:\n",
    "                    self.feature_counts[label][i] = {}\n",
    "                if value not in self.feature_counts[label][i]:\n",
    "                    self.feature_counts[label][i][value] = 0\n",
    "                self.feature_counts[label][i][value] += 1\n",
    "                self.feature_totals[label] += 1\n",
    "\n",
    "    def _class_probability(self, x, c):\n",
    "        \"\"\"\n",
    "        Compute the log probability of class `c` given input sample `x` using Naïve Bayes formula.\n",
    "        Steps:\n",
    "        1. Compute the log prior probability of class `c`.\n",
    "        2. Compute the log likelihood of the input sample `x` given class `c`.\n",
    "        3. Sum log prior and log likelihood to get the final log probability.\n",
    "        \n",
    "        :param x: 1D list or numpy array representing a single sample.\n",
    "        :param c: Class label for which probability is being computed.\n",
    "        :return: Computed log probability of `c` given `x`.\n",
    "        \"\"\"\n",
    "        # Step 1: Compute log prior probability log(P(C))\n",
    "        # log_prior = ...\n",
    "        log_prior = np.log(self.class_counts[c] / sum(self.class_counts.values()))\n",
    "        \n",
    "        # Step 2: Compute log likelihood log(P(X|C)) by apply laplace smoothing\n",
    "        log_likelihood = 0\n",
    "        # ...\n",
    "        for i, value in enumerate(x):\n",
    "            if value not in self.feature_counts[c][i]:\n",
    "                self.feature_counts[c][i][value] = 0\n",
    "            log_likelihood += np.log((self.feature_counts[c][i][value] + 1) / (self.feature_totals[c] + len(self.feature_counts[c][i])))\n",
    "        \n",
    "        # Step 3: Compute final log probability log(P(C|X)) (ignoring P(X))\n",
    "        return log_prior + log_likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for given input `X`.\n",
    "        Steps:\n",
    "        1. Compute the log probability for each class using `_class_probability`.\n",
    "        2. Choose the class with the highest log probability as the prediction.\n",
    "        \n",
    "        :param X: 2D list or numpy array where each row represents a sample.\n",
    "        :return: 1D numpy array of predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Step 1: Compute log probability for each class\n",
    "            # ...\n",
    "            log_probs = [self._class_probability(x, c) for c in self.classes]\n",
    "\n",
    "            # Step 2: Choose the class with the highest probability\n",
    "            # ...\n",
    "            y_pred.append(self.classes[np.argmax(log_probs)])\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n",
      "Expected prediction: [0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[1, 2], [1, 1], [2, 3], [5, 4], [5, 3], [6, 5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "X_test = np.array([[1, 2], [5, 4]])\n",
    "\n",
    "# Train model\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = nb.predict(X_test)\n",
    "print(\"Predictions:\", predictions) \n",
    "print(\"Expected prediction: [0 1]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Nested cross validation\n",
    "#### **Objective**\n",
    "In this lab, you will implement missing parts of a `NestedCrossValidator` class. This class performs nested cross-validation for hyperparameter tuning and model evaluation.\n",
    "\n",
    "#### **Overview**\n",
    "Nested cross-validation consists of two levels:\n",
    "1. **Outer Cross-Validation**: Splits the dataset into training and test sets to evaluate model performance.\n",
    "2. **Inner Cross-Validation**: Further splits the training data to perform hyperparameter tuning.\n",
    "\n",
    "Your task is to implement the missing parts (marked as `TODO`) in the given code. Note that the\n",
    "\n",
    "---\n",
    "\n",
    "#### **SubTasks**\n",
    "\n",
    "##### **SubTask 1: Implement Manual Fold Creation**\n",
    "- **Function to complete**: `_create_folds(self, X, y, n_splits, data_fraction=1.0)`\n",
    "- **Purpose**: Manually create train-test folds for cross-validation. But this function behave a little bit different. Sometimes we have too many data and we want to use a little bit smaller part of the data. So we have data_fraction parameter. If we have **N** samples in total, then we actually use **int(data_fraction*N)** samples.\n",
    "- **Implementation details**:\n",
    "  - Ensure `data_fraction` is between 0.0 and 1.0.\n",
    "  - Select a subset of indices based on `data_fraction`.\n",
    "  - Shuffle the selected indices.\n",
    "  - Split them into `n_splits` folds.\n",
    "  - Return a list of `(train_indices, test_indices)`.\n",
    "\n",
    "---\n",
    "\n",
    "##### **SubTask 2: Implement Nested Cross-Validation**\n",
    "- **Function to complete**: `fit(self, X, y, outer_data_fraction=1.0, inner_data_fraction=0.5)`\n",
    "- **Purpose**: Perform nested cross-validation for model selection and evaluation.\n",
    "- **Implementation details**:\n",
    "  - Create outer folds using `_create_folds()`.\n",
    "  - Iterate through outer folds:\n",
    "    - Split data into outer train/test sets.\n",
    "    - Initialize best parameters and best score.\n",
    "    - Create inner folds from outer training set.\n",
    "    - Iterate over hyperparameter combinations:\n",
    "      - Train model using inner train sets.\n",
    "      - Evaluate model using inner validation sets.\n",
    "      - Store the best hyperparameter combination.\n",
    "    - Train the final model on outer training data using the best parameters.\n",
    "    - Evaluate the model on the outer test set.\n",
    "  - Return list of outer fold accuracies and overall mean accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "\n",
    "class NestedCrossValidator:\n",
    "    def __init__(self, model_class, param_grid, outer_splits=5, inner_splits=3, random_seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the nested cross-validator.\n",
    "\n",
    "        :param model_class: The model class to be used (e.g., SVC, RandomForestClassifier).\n",
    "        :param param_grid: Dictionary of hyperparameters and their values to search.\n",
    "        :param outer_splits: Number of splits for the outer cross-validation.\n",
    "        :param inner_splits: Number of splits for the inner cross-validation.\n",
    "        :param random_seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.param_grid = param_grid\n",
    "        self.outer_splits = outer_splits\n",
    "        self.inner_splits = inner_splits\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def _create_folds(self, X, y, n_splits, data_fraction=1.0):\n",
    "        \"\"\"\n",
    "        Manually create folds for cross-validation.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector.\n",
    "        :param n_splits: Number of splits for cross-validation.\n",
    "        :param data_fraction: Fraction of data to use (between 0.0 and 1.0).\n",
    "        :return: List of (train_indices, test_indices) for each fold.\n",
    "        \"\"\"\n",
    "        # Validate data_fraction\n",
    "        if not 0.0 < data_fraction <= 1.0:\n",
    "            raise ValueError(\"data_fraction must be between 0.0 and 1.0 (exclusive).\")\n",
    "\n",
    "        np.random.seed(self.random_seed) # DO NOT DELETE THIS LINE\n",
    "        total_indices = np.arange(len(X))\n",
    "        subset_size = int(len(X) * data_fraction)\n",
    "        # TODO: implement this function\n",
    "        # Step 1: Select a subset of data indices based on data_fraction. (use np.random.choice function)\n",
    "        # selected_indices = ...\n",
    "        selected_indices = np.random.choice(total_indices, subset_size, replace=False)\n",
    "\n",
    "        # Step 2: Shuffle the selected indices. (use np.random.shuffle function)\n",
    "        # ...\n",
    "        np.random.shuffle(selected_indices)\n",
    "\n",
    "        # Step 3: Calculate the size for each fold split\n",
    "        # fold_size = ...\n",
    "        folds = []\n",
    "        fold_size = len(selected_indices) // n_splits\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            # Step 3: Split the shuffled indices into n_splits folds. \n",
    "            test_indices = selected_indices[i * fold_size:(i + 1) * fold_size]\n",
    "            # train_indices = ...\n",
    "            train_indices = np.setdiff1d(selected_indices, test_indices)\n",
    "            folds.append((train_indices, test_indices))\n",
    "\n",
    "        return folds\n",
    "\n",
    "    def fit(self, X, y, outer_data_fraction=1.0, inner_data_fraction=0.5):\n",
    "        \"\"\"\n",
    "        Perform nested cross-validation to evaluate model performance.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector.\n",
    "        :param outer_data_fraction: Fraction of data to use for outer loop.\n",
    "        :param inner_data_fraction: Fraction of data to use for inner loop.\n",
    "        :return: List of outer fold accuracies and mean accuracy.\n",
    "        \"\"\"\n",
    "        # TODO: implement this function\n",
    "        # Step 1: Create outer folds using _create_folds().\n",
    "        # outer_folds = ...\n",
    "        outer_folds = self._create_folds(X, y, self.outer_splits, outer_data_fraction)\n",
    "        # Step 2: Initialize a list to store outer fold results.\n",
    "        outer_results = []\n",
    "\n",
    "        # Step 3: Loop through each outer fold.\n",
    "        for outer_idx, (train_idx, test_idx) in enumerate(outer_folds):\n",
    "            # Step 4: Split data into outer train/test sets.\n",
    "            # ...\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "\n",
    "            # Inner loop for hyperparameter tuning\n",
    "            # Step 5: Initialize best parameters and best score.\n",
    "            best_params = None\n",
    "            best_score = -np.inf\n",
    "\n",
    "            # Step 6: Create inner folds from outer training data. (Don't forget inner_data_fraction here)\n",
    "            # inner_folds = ...\n",
    "            inner_folds = self._create_folds(X_train, y[train_idx], self.inner_splits, inner_data_fraction)\n",
    "\n",
    "            # Step 7: Iterate over hyperparameter combinations.\n",
    "            for param_combination in product(*self.param_grid.values()):\n",
    "                params = dict(zip(self.param_grid.keys(), param_combination))\n",
    "                inner_scores = []\n",
    "\n",
    "                # Step 8: Train model using inner train sets and evaluate using validation sets.\n",
    "                for inner_train_idx, inner_val_idx in inner_folds:\n",
    "                    # Split data into inner train/test sets\n",
    "                    # ...\n",
    "                    X_inner_train, X_inner_val = X_train[inner_train_idx], X_train[inner_val_idx]\n",
    "\n",
    "                    model = self.model_class(**params)\n",
    "                    # Train model with the current parameters\n",
    "                    # ...\n",
    "                    model.fit(X_inner_train, y[train_idx][inner_train_idx])\n",
    "\n",
    "                    # Evaluate on validation set and update inner_scores\n",
    "                    # ...\n",
    "                    y_pred = model.predict(X_inner_val)\n",
    "\n",
    "                # Step 9: Select the best hyperparameter combination.\n",
    "                # Compute mean score and update best parameters\n",
    "                # ...\n",
    "                score = accuracy_score(y[train_idx][inner_val_idx], y_pred)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "\n",
    "            # Step 10: Train the final model on outer training data using best parameters.\n",
    "            # final_model = ...\n",
    "            # ...\n",
    "            final_model = self.model_class(**best_params)\n",
    "            final_model.fit(X_train, y[train_idx])\n",
    "\n",
    "            # Step 11: Evaluate the final model on outer test set and store the result.\n",
    "            # ...\n",
    "            # outer_accuracy = ...\n",
    "            # ...\n",
    "            y_pred = final_model.predict(X_test)\n",
    "            outer_accuracy = accuracy_score(y[test_idx], y_pred)\n",
    "            outer_results.append(outer_accuracy)\n",
    "\n",
    "            print(f\"Outer Fold {outer_idx + 1} - Best Params: {best_params}, Accuracy: {outer_accuracy:.4f}\")\n",
    "\n",
    "        mean_outer_accuracy = np.mean(outer_results)\n",
    "        print(f\"\\nOverall Accuracy from Nested Cross-Validation: {mean_outer_accuracy:.4f}\")\n",
    "        # Step 12: Return list of outer fold accuracies and overall mean accuracy.\n",
    "        return outer_results, mean_outer_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 - Best Params: {'C': 10, 'kernel': 'rbf'}, Accuracy: 0.9667\n",
      "Outer Fold 2 - Best Params: {'C': 1, 'kernel': 'linear'}, Accuracy: 1.0000\n",
      "Outer Fold 3 - Best Params: {'C': 1, 'kernel': 'linear'}, Accuracy: 0.9667\n",
      "Outer Fold 4 - Best Params: {'C': 1, 'kernel': 'linear'}, Accuracy: 1.0000\n",
      "Outer Fold 5 - Best Params: {'C': 0.1, 'kernel': 'linear'}, Accuracy: 1.0000\n",
      "\n",
      "Overall Accuracy from Nested Cross-Validation: 0.9867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Initialize and run nested cross-validation\n",
    "nested_cv = NestedCrossValidator(model_class=SVC, param_grid=param_grid, outer_splits=5, inner_splits=3, random_seed=0)\n",
    "outer_results, meanaccuracy = nested_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output in my laptop is:\n",
    "```\n",
    "Outer Fold 1 - Best Params: {'C': 10, 'kernel': 'rbf'}, Accuracy: 0.9667\n",
    "Outer Fold 2 - Best Params: {'C': 1, 'kernel': 'linear'}, Accuracy: 1.0000\n",
    "Outer Fold 3 - Best Params: {'C': 10, 'kernel': 'rbf'}, Accuracy: 0.9667\n",
    "Outer Fold 4 - Best Params: {'C': 0.1, 'kernel': 'linear'}, Accuracy: 0.9333\n",
    "Outer Fold 5 - Best Params: {'C': 0.1, 'kernel': 'linear'}, Accuracy: 1.0000\n",
    "\n",
    "Overall Accuracy from Nested Cross-Validation: 0.9733\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Acan_Python_3_12_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
